# Lessons Learned - Why Improvements Failed

## âŒ What Happened

**Attempted Improvements:**
1. Reward weight: 0.5 â†’ 2.0 
2. State features: 6 â†’ 14
3. Hidden units: 128 â†’ 256
4. Gamma: 0.95 â†’ 0.99

**Result:** Performance DEGRADED instead of improved

---

## ðŸ§  Why Multiple Changes Failed

### Problem: Changed Too Many Things at Once
- Can't identify which change caused the problem
- Changes might conflict with each other
- Network size + state size = much harder to train
- Different normalization broke the learning

### Specific Issues:

**1. State Normalization Mismatch**
- Old: Raw values (queues 0-20)
- New: Normalized values (0-1)
- Network expects certain input ranges
- Breaking that confused the learning

**2. Network Too Big Too Fast**
- 128 â†’ 256 neurons needs more data
- More parameters = harder to optimize
- Needs longer training or different learning rate

**3. Gamma Too High**
- 0.95 â†’ 0.99 changes TD error magnitude
- Needs re-tuning of learning rate
- Can cause instability without other adjustments

**4. Reward Weight Too Aggressive**
- 0.5 â†’ 2.0 is a 4x change
- Completely changes reward scale
- Agent's learned Q-values became invalid

---

## âœ… The RIGHT Approach

### **ONE CHANGE AT A TIME**

#### Step 1: Extended Training (Safest)
```powershell
# Just train the proven baseline longer
$env:PYTHONDONTWRITEBYTECODE="1"; python main.py --mode train --episodes 1000
```
**Pros:**
- âœ… Zero risk
- âœ… Baseline was still improving at episode 500
- âœ… Expected +10-15% gain
- âœ… Takes ~2.5 hours

**After:** Save with `python save_extended.py`

---

#### Step 2: Try ONE Small Change at a Time

**Option A: Gentle Reward Tuning**
```python
# sumo_environment.py, line ~201
reward = -total_queue - 0.75 * total_waiting  # Just 0.5 â†’ 0.75 (not 2.0!)
```
Train 500 episodes, evaluate, compare.

**Option B: Slightly Bigger Network**
```python
# main.py
HIDDEN_DIM = 160  # Just 128 â†’ 160 (not 256!)
```
Train 500 episodes, evaluate, compare.

**Option C: Better Gamma**
```python
# main.py
GAMMA = 0.97  # Just 0.95 â†’ 0.97 (not 0.99!)
```
Train 500 episodes, evaluate, compare.

---

## ðŸ“Š Proper Experimentation Workflow

```
1. Baseline (500ep) âœ… DONE
   â†“
2. Extended Baseline (1000ep) â† START HERE
   â†“
3. If still improving, try 1500ep
   â†“
4. Once converged, try ONE small change:
   â†“
   â†’ Test reward weight 0.75
   â†’ Save & evaluate
   â†’ If better, keep it. If worse, revert.
   â†“
5. Try next change on BEST model so far
   â†“
6. Repeat until satisfied
```

---

## ðŸŽ¯ Recommended Next Steps

### **Option 1: Safe & Proven (RECOMMENDED)**
Train the reverted baseline for 1000 episodes:

```powershell
$env:PYTHONDONTWRITEBYTECODE="1"; python main.py --mode train --episodes 1000
```

After completion:
```powershell
python save_extended.py
```

**Expected result:** -6,670 â†’ -5,500 to -6,000 (better than baseline)

---

### **Option 2: Quick Improvement Test**
Make ONE small change and train 500 episodes:

**Edit `sumo_environment.py` line ~201:**
```python
reward = -total_queue - 0.75 * total_waiting  # Small increase from 0.5
```

**Train:**
```powershell
$env:PYTHONDONTWRITEBYTECODE="1"; python main.py --mode train --episodes 500
```

**Save:**
```python
from experiment_manager import save_current_training
save_current_training('reward_075', 'Gentle reward tune: 0.5â†’0.75', {'reward_weight': 0.75})
```

**Compare:**
```python
from experiment_manager import ExperimentManager
manager = ExperimentManager()
manager.compare_experiments(['baseline_500ep_GPU_20260205_235635', 'reward_075_TIMESTAMP'])
```

---

## ðŸ“ˆ What We Learned

### âœ… Good Practices:
1. **ONE change at a time**
2. **Small incremental changes** (0.5 â†’ 0.75, not 0.5 â†’ 2.0)
3. **Save every experiment** with descriptive names
4. **Compare before moving on**
5. **Keep proven baseline safe**

### âŒ Bad Practices:
1. Changing 4 things simultaneously
2. Making large jumps (128 â†’ 256, 0.5 â†’ 2.0)
3. Changing normalization without re-tuning
4. Not testing intermediate steps

---

## ðŸ”„ Current Status

**Your Safe Models:**
- âœ… `experiments/baseline_500ep_GPU_20260205_235635/` (Avg reward: -6,670)
  - Can test anytime with this model
  - Proven to work well

**Code Status:**
- âœ… REVERTED to baseline configuration
- âœ… Ready to train 1000 episodes safely
- âœ… All improvements removed

**Next Action:**
Train 1000 episodes with proven config for guaranteed improvement!

---

## ðŸ’¡ Key Insight

**Simple is better than complex.**

Your baseline with 6 features and 128 neurons achieved **61% improvement** in 500 episodes. That's excellent! 

Rather than making it "smarter" with more features, just give it **more time to learn** what it already knows works.

**1000 episodes of baseline > 500 episodes of complex model**

---

**Recommendation: Run `train_extended.py` or the command below:**

```powershell
$env:PYTHONDONTWRITEBYTECODE="1"; python main.py --mode train --episodes 1000
```

This is the safest path to better results! ðŸŽ¯
